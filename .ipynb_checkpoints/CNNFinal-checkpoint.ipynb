{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found a different version 3.0.0 of dataset mnist in data_dir /home/nirisha/tensorflow_datasets. Using currently defined version 1.0.0.\n"
     ]
    }
   ],
   "source": [
    "dataset, info = tfds.load('mnist',as_supervised = True,split =  [\n",
    "        tfds.Split.TRAIN.subsplit(tfds.percent[:80]),\n",
    "        tfds.Split.TRAIN.subsplit(tfds.percent[80:90]),\n",
    "        tfds.Split.TRAIN.subsplit(tfds.percent[90:]),\n",
    "    ],\n",
    "    with_info = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<_OptionsDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>,\n",
       " <_OptionsDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>,\n",
       " <_OptionsDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[0]\n",
    "test_dataset = dataset[1]\n",
    "valid_dataset = dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'> <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'> <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'> <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'> <class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "for image,label in train_dataset.batch(16).take(4):\n",
    "    print(type(image),type(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    version=1.0.0,\n",
       "    description='The MNIST database of handwritten digits.',\n",
       "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "    }),\n",
       "    total_num_examples=70000,\n",
       "    splits={\n",
       "        'test': 10000,\n",
       "        'train': 60000,\n",
       "    },\n",
       "    supervised_keys=('image', 'label'),\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image,label):\n",
    "    #to grayscale\n",
    "    image = tf.dtypes.cast(image, tf.float32)\n",
    "    label = tf.dtypes.cast(label, tf.float32)\n",
    "    \n",
    "    #resize\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    \n",
    "    return image,label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nirisha/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nirisha/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(preprocess)\n",
    "test_dataset = test_dataset.map(preprocess)\n",
    "valid_dataset = valid_dataset.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 28, 28, 1) (16,) tf.Tensor([8. 8. 3. 0. 0. 4. 0. 6. 1. 1. 7. 1. 9. 5. 0. 6.], shape=(16,), dtype=float32)\n",
      "(16, 28, 28, 1) (16,) tf.Tensor([1. 2. 0. 1. 9. 4. 3. 0. 4. 6. 6. 8. 3. 4. 0. 3.], shape=(16,), dtype=float32)\n",
      "(16, 28, 28, 1) (16,) tf.Tensor([3. 0. 7. 5. 6. 4. 4. 2. 8. 2. 9. 2. 2. 1. 3. 1.], shape=(16,), dtype=float32)\n",
      "(16, 28, 28, 1) (16,) tf.Tensor([2. 2. 7. 5. 1. 9. 5. 6. 3. 3. 2. 1. 0. 0. 5. 1.], shape=(16,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for image, label in train_dataset.batch(16).take(4):\n",
    "    print(image.shape, label.shape, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        initializer = tf.initializers.GlorotUniform(seed=123)\n",
    "        # Conv1\n",
    "        self.wc1 = tf.Variable(initializer([3, 3, 1, 6]), trainable=True, name='wc1')\n",
    "        \n",
    "        # Conv2\n",
    "        self.wc2 = tf.Variable(initializer([3, 3, 6, 16]), trainable=True, name='wc2')\n",
    "        \n",
    "        # Flatten\n",
    "        \n",
    "        # Dense\n",
    "        self.wd3 = tf.Variable(initializer([400, 128]), trainable=True)\n",
    "        self.wd4 = tf.Variable(initializer([128, 64]), trainable=True)        \n",
    "        self.wd5 = tf.Variable(initializer([64, 10]), trainable=True)\n",
    "        \n",
    "        self.bc1 = tf.Variable(tf.zeros([6]), dtype=tf.float32, trainable=True)\n",
    "        self.bc2 = tf.Variable(tf.zeros([16]), dtype=tf.float32, trainable=True)\n",
    "        self.bd3 = tf.Variable(tf.zeros([128]), dtype=tf.float32, trainable=True)\n",
    "        self.bd4 = tf.Variable(tf.zeros([64]), dtype=tf.float32, trainable=True)        \n",
    "        self.bd5 = tf.Variable(tf.zeros([10]), dtype=tf.float32, trainable=True)   \n",
    "    \n",
    "    def call(self, x):\n",
    "        # X = NHWC \n",
    "        # Conv1 + maxpool 2\n",
    "        x = tf.nn.conv2d(x, self.wc1, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "        x = tf.nn.bias_add(x, self.bc1)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "        \n",
    "        # Conv2 + maxpool 2\n",
    "        x = tf.nn.conv2d(x, self.wc2, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "        x = tf.nn.bias_add(x, self.bc2)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "        \n",
    "        # Flattten out\n",
    "        # N X Number of Nodes\n",
    "        # Flatten()\n",
    "        x = tf.reshape(x, (tf.shape(x)[0], -1))\n",
    "        \n",
    "        # Dense1\n",
    "        x = tf.matmul(x, self.wd3)\n",
    "        x = tf.nn.bias_add(x, self.bd3)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        \n",
    "        # Dense2\n",
    "        x = tf.matmul(x, self.wd4)\n",
    "        x = tf.nn.bias_add(x, self.bd4)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "        \n",
    "        # Dense3\n",
    "        x = tf.matmul(x, self.wd5)\n",
    "        x = tf.nn.bias_add(x, self.bd5)\n",
    "#         x = tf.nn.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate= LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, labels, loss_fn, optimzer):\n",
    "    with tf.GradientTape() as t:\n",
    "        y_predicted = model(inputs, training=True)\n",
    "        current_loss = loss_fn(labels, y_predicted)\n",
    "        \n",
    "        gradients = t.gradient(current_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "    return current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_step(model, inputs, labels, loss_fn):\n",
    "    y_predicted = model(inputs, training=False)\n",
    "    current_loss = loss_fn(labels, y_predicted)\n",
    "    return current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = tf.keras.metrics.Mean(name='loss')\n",
    "val_losses = tf.keras.metrics.Mean(name='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = f'./temp/train/{current_time}/logs'\n",
    "file_writer = tf.summary.create_file_writer(train_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint\n",
    "# callbacks = [tf.keras.Checkpoint(...)]\n",
    "checkpoint_dir = f'./temp/train/checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(step = tf.Variable(1), optimizer=optimizer, net=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, checkpoint_dir, max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_checkpoint(manager):\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(f\"restored from {manager.latest_checkpoint}\")\n",
    "    else:\n",
    "        print(\"Initializing from scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored from ./temp/train/checkpoints/ckpt-3\n"
     ]
    }
   ],
   "source": [
    "check_for_checkpoint(manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Saved checkpoint for step 6: ./temp/train/checkpoints/ckpt-4\n",
      "tf.Tensor(0.08000128, shape=(), dtype=float32)\n",
      "epoch: 1\n",
      "Saved checkpoint for step 7: ./temp/train/checkpoints/ckpt-5\n",
      "tf.Tensor(0.06830396, shape=(), dtype=float32)\n",
      "epoch: 2\n",
      "Saved checkpoint for step 8: ./temp/train/checkpoints/ckpt-6\n",
      "tf.Tensor(0.059911564, shape=(), dtype=float32)\n",
      "epoch: 3\n",
      "Saved checkpoint for step 9: ./temp/train/checkpoints/ckpt-7\n",
      "tf.Tensor(0.053467274, shape=(), dtype=float32)\n",
      "epoch: 4\n",
      "Saved checkpoint for step 10: ./temp/train/checkpoints/ckpt-8\n",
      "tf.Tensor(0.0481898, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    ckpt.step.assign_add(1)\n",
    "\n",
    "    print(f'epoch: {epoch}')\n",
    "    losses.reset_states()\n",
    "    val_losses.reset_states()\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        loss = train_step(model, x_batch, y_batch, ce_loss, optimizer)\n",
    "        losses(loss)\n",
    "#         step += 1\n",
    "    \n",
    "    save_path = manager.save()\n",
    "    print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n",
    "\n",
    "    with file_writer.as_default():\n",
    "        tf.summary.scalar('loss', losses.result(), step=epoch)\n",
    "        tf.summary.image('Input images', x_batch, step=epoch)\n",
    "\n",
    "    print(losses.result())\n",
    "        \n",
    "    for x_batch, y_batch in valid_dataset:\n",
    "        val_loss = valid_step(model, x_batch, y_batch, ce_loss)\n",
    "        val_losses(val_loss)\n",
    "    \n",
    "    with file_writer.as_default():\n",
    "        tf.summary.scalar('val_loss', val_losses.result(), step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs):\n",
    "    predicted = model(inputs)\n",
    "    return tf.nn.softmax(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([9 5 8 0 4 1 4 9 8 4 8 7 9 3 8 0], shape=(16,), dtype=int64) tf.Tensor([9. 5. 6. 0. 4. 1. 4. 9. 8. 4. 8. 7. 9. 3. 8. 0.], shape=(16,), dtype=float32)\n",
      "tf.Tensor([0 4 3 2 0 5 7 3 9 4 7 4 7 6 9 8], shape=(16,), dtype=int64) tf.Tensor([0. 4. 3. 2. 0. 5. 7. 3. 9. 4. 7. 4. 7. 6. 9. 8.], shape=(16,), dtype=float32)\n",
      "tf.Tensor([3 9 3 6 5 0 3 8 9 8 0 2 9 0 9 9], shape=(16,), dtype=int64) tf.Tensor([3. 9. 3. 6. 5. 0. 3. 8. 9. 8. 0. 2. 9. 0. 9. 9.], shape=(16,), dtype=float32)\n",
      "tf.Tensor([1 5 9 5 9 3 3 2 9 7 0 8 0 3 8 5], shape=(16,), dtype=int64) tf.Tensor([1. 5. 9. 5. 9. 3. 3. 2. 9. 7. 0. 8. 0. 3. 8. 5.], shape=(16,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for sample, label in test_dataset.batch(16).take(4):\n",
    "    predictions = predict(sample)\n",
    "    print(tf.argmax(predictions, axis=1), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = f'./temp/train/models/'\n",
    "weights_path = os.path.join(model_dir, 'weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
